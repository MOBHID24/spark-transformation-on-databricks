# Data Processing Pipeline with AWS S3, Databricks, Spark, Seaborn, and Matplotlib
## Overview
![image](https://github.com/MOBHID24/spark-transformation-on-databricks/assets/155905040/ad4891b1-9a3e-49d2-8b7f-cb5f28363e9b)


This project implements a data processing pipeline that involves loading files manually from local storage into AWS S3 buckets, accessing them from Databricks, performing transformations using Apache Spark, and visualizing the results using Seaborn and Matplotlib within the Databricks environment.

## Key Components
#### Data Ingestion: Files are manually loaded into AWS S3 buckets from local storage.
#### AWS S3: Serves as the central data repository for storing the files.
#### Databricks: The data analytics platform used for its unified analytics environment and collaborative workspace.
#### Apache Spark: Utilized within Databricks for data processing and analysis.
#### Seaborn and Matplotlib: Python libraries used for data visualization.
## Project Workflow
#### Data Loading: Files are uploaded manually to designated AWS S3 buckets.
#### Data Access: Databricks connects to the AWS S3 buckets to access the stored data.
#### Data Processing: Spark jobs are created within Databricks to perform various data transformations.
#### Visualization: The processed data is visualized using Seaborn and Matplotlib within Databricks notebooks.
## Requirements
#### AWS account with permissions to access S3 buckets.
#### Databricks account for running Spark jobs and notebooks.
#### Python environment with Seaborn and Matplotlib installed.
## Usage
 Data Loading:Manually upload the required files to the designated AWS S3 buckets.
 Data Processing:
 Access Databricks environment.
 Create Spark jobs or notebooks to perform data transformations as needed.
 Visualization:Utilize Seaborn and Matplotlib within Databricks notebooks to visualize the processed data.
